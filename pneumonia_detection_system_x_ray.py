# -*- coding: utf-8 -*-
"""Pneumonia Detection System x-ray.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/189UVaYyZdLLDI2BNy6I3WHmnMRAj70t2
"""

from IPython.display import display
print("Cleaned for GitHub rendering")

!git clone https://github.com/AS190-web/Pneumonia-Detection-Project-using-basic-DL-Concept-and-Kaggle-Datasets

# Commented out IPython magic to ensure Python compatibility.
# %cd Pneumonia-Detection-Project-using-basic-DL-Concept-and-Kaggle-Datasets
!git add
!git commit -m "Notebook Added"
!git push origin main

import os
import random
import numpy as np
import matplotlib.pyplot as plt
import tensorflow as tf
import tensorflow_datasets as tfds
from sklearn.metrics import accuracy_score, confusion_matrix, roc_curve, auc, classification_report

SEED = 42
np.random.seed(SEED)
tf.random.set_seed(SEED)
random.seed(SEED)

(ds_train, ds_test), ds_info = tfds.load(
    "pneumonia_mnist",
    split=["train", "test"],
    as_supervised=True,
    with_info=True,
)
LABELS = ds_info.features['label'].names
print("Labels:", LABELS)

IMG_SIZE = (64, 64)
BATCH = 32

def preprocess(image, label):
    image = tf.image.resize(image, IMG_SIZE) / 255.0
    image = tf.reshape(image, [IMG_SIZE[0], IMG_SIZE[1], 1])
    return image, label

train_ds = ds_train.map(preprocess).shuffle(512).batch(BATCH).prefetch(tf.data.AUTOTUNE)
test_ds  = ds_test.map(preprocess).batch(BATCH).prefetch(tf.data.AUTOTUNE)

# Collect test images/labels to numpy arrays for evaluation/plots
imgs_list, y_list = [], []
for imgs, labs in test_ds:
    imgs_list.append(imgs.numpy())
    y_list.append(labs.numpy())
imgs_all = np.concatenate(imgs_list, axis=0)
y_all = np.concatenate(y_list, axis=0)
N = len(y_all)
print("Test set size:", N)

def build_simple_cnn():
    inp = tf.keras.Input(shape=(IMG_SIZE[0], IMG_SIZE[1], 1))
    x = tf.keras.layers.Conv2D(32, 3, activation='relu', padding='same')(inp)
    x = tf.keras.layers.MaxPooling2D()(x)
    x = tf.keras.layers.Conv2D(64, 3, activation='relu', padding='same')(x)
    x = tf.keras.layers.MaxPooling2D()(x)
    x = tf.keras.layers.Flatten()(x)
    x = tf.keras.layers.Dense(128, activation='relu')(x)
    x = tf.keras.layers.Dropout(0.4)(x)
    out = tf.keras.layers.Dense(1, activation='sigmoid')(x)
    model = tf.keras.Model(inp, out)
    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
    return model

def resnet_block(x, filters, kernel=3):
    y = tf.keras.layers.Conv2D(filters, kernel, padding='same', activation='relu')(x)
    y = tf.keras.layers.Conv2D(filters, kernel, padding='same')(y)
    y = tf.keras.layers.Add()([x, y])
    y = tf.keras.layers.Activation('relu')(y)
    return y

def build_resnet_like():
    inp = tf.keras.Input(shape=(IMG_SIZE[0], IMG_SIZE[1], 1))
    x = tf.keras.layers.Conv2D(32, 3, padding='same', activation='relu')(inp)
    x = tf.keras.layers.MaxPooling2D()(x)
    # small residual block
    x = resnet_block(x, 32)
    x = tf.keras.layers.MaxPooling2D()(x)
    x = tf.keras.layers.Flatten()(x)
    x = tf.keras.layers.Dense(64, activation='relu')(x)
    x = tf.keras.layers.Dropout(0.4)(x)
    out = tf.keras.layers.Dense(1, activation='sigmoid')(x)
    model = tf.keras.Model(inp, out)
    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
    return model

def dense_block(x, filters):
    # simple dense block: conv -> concat
    y = tf.keras.layers.Conv2D(filters, 3, padding='same', activation='relu')(x)
    out = tf.keras.layers.Concatenate()([x, y])
    return out

def build_densenet_like():
    inp = tf.keras.Input(shape=(IMG_SIZE[0], IMG_SIZE[1], 1))
    x = tf.keras.layers.Conv2D(16, 3, padding='same', activation='relu')(inp)
    x = dense_block(x, 16)
    x = tf.keras.layers.MaxPooling2D()(x)
    x = dense_block(x, 32)
    x = tf.keras.layers.GlobalAveragePooling2D()(x)
    x = tf.keras.layers.Dense(64, activation='relu')(x)
    x = tf.keras.layers.Dropout(0.4)(x)
    out = tf.keras.layers.Dense(1, activation='sigmoid')(x)
    model = tf.keras.Model(inp, out)
    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
    return model

def build_vgg_like():
    inp = tf.keras.Input(shape=(IMG_SIZE[0], IMG_SIZE[1], 1))
    x = tf.keras.layers.Conv2D(32, (3,3), activation='relu', padding='same')(inp)
    x = tf.keras.layers.Conv2D(32, (3,3), activation='relu', padding='same')(x)
    x = tf.keras.layers.MaxPooling2D()(x)
    x = tf.keras.layers.Conv2D(64, (3,3), activation='relu', padding='same')(x)
    x = tf.keras.layers.Conv2D(64, (3,3), activation='relu', padding='same')(x)
    x = tf.keras.layers.MaxPooling2D()(x)
    x = tf.keras.layers.Flatten()(x)
    x = tf.keras.layers.Dense(128, activation='relu')(x)
    x = tf.keras.layers.Dropout(0.4)(x)
    out = tf.keras.layers.Dense(1, activation='sigmoid')(x)
    model = tf.keras.Model(inp, out)
    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
    return model

def inception_module(x, filters):
    # tiny inception-like module
    p1 = tf.keras.layers.Conv2D(filters, 1, padding='same', activation='relu')(x)
    p2 = tf.keras.layers.Conv2D(filters, 3, padding='same', activation='relu')(x)
    p3 = tf.keras.layers.Conv2D(filters, 5, padding='same', activation='relu')(x)
    p4 = tf.keras.layers.MaxPooling2D(pool_size=3, strides=1, padding='same')(x)
    p4 = tf.keras.layers.Conv2D(filters, 1, padding='same', activation='relu')(p4)
    return tf.keras.layers.Concatenate()([p1, p2, p3, p4])

def build_inception_like():
    inp = tf.keras.Input(shape=(IMG_SIZE[0], IMG_SIZE[1], 1))
    x = tf.keras.layers.Conv2D(16, 3, padding='same', activation='relu')(inp)
    x = inception_module(x, 16)
    x = tf.keras.layers.MaxPooling2D()(x)
    x = inception_module(x, 32)
    x = tf.keras.layers.GlobalAveragePooling2D()(x)
    x = tf.keras.layers.Dense(64, activation='relu')(x)
    x = tf.keras.layers.Dropout(0.4)(x)
    out = tf.keras.layers.Dense(1, activation='sigmoid')(x)
    model = tf.keras.Model(inp, out)
    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
    return model

models = {
    'SimpleCNN': build_simple_cnn(),
    'ResNetLike': build_resnet_like(),
    'DenseNetLike': build_densenet_like(),
    'VGGLike': build_vgg_like(),
    'InceptionLike': build_inception_like()
}

# Print summaries briefly
for name, m in models.items():
    print(f"\n--- {name} ---")
    m.summary()

EPOCHS = 2
histories = {}
for name, model in models.items():
    print(f"\nTraining {name} for {EPOCHS} epochs...")
    histories[name] = model.fit(train_ds, validation_data=test_ds, epochs=EPOCHS, verbose=1)

probs_dict = {}
accs = {}
for name, model in models.items():
    probs = model.predict(imgs_all, batch_size=64, verbose=0).ravel()
    preds = (probs >= 0.5).astype(int)
    acc = accuracy_score(y_all, preds)
    probs_dict[name] = probs
    accs[name] = acc
    print(f"{name} accuracy (0.5): {acc:.4f}")

ensemble_probs = np.mean(np.stack(list(probs_dict.values()), axis=0), axis=0)
ensemble_preds = (ensemble_probs >= 0.5).astype(int)
ensemble_acc = accuracy_score(y_all, ensemble_preds)
print(f"\nEnsemble (avg probs) accuracy (0.5): {ensemble_acc:.4f}")

fpr, tpr, thr = roc_curve(y_all, ensemble_probs)
roc_auc = auc(fpr, tpr)
print(f"Ensemble ROC AUC: {roc_auc:.4f}")

plt.figure(figsize=(6,5))
plt.plot(fpr, tpr, label=f'AUC={roc_auc:.3f}')
plt.plot([0,1],[0,1],'--', color='gray')
plt.xlabel('False Positive Rate'); plt.ylabel('True Positive Rate')
plt.title('Ensemble ROC')
plt.legend(); plt.grid(True); plt.show()

cm = confusion_matrix(y_all, ensemble_preds)
plt.figure(figsize=(4,3))
plt.imshow(cm, cmap='Blues', interpolation='nearest')
plt.colorbar()
plt.xticks([0,1], LABELS, rotation=45)
plt.yticks([0,1], LABELS)
plt.title('Confusion Matrix (Ensemble at 0.5)')
th = cm.max()/2.
for i in range(cm.shape[0]):
    for j in range(cm.shape[1]):
        plt.text(j, i, int(cm[i,j]), ha='center',
                 color='white' if cm[i,j] > th else 'black')
plt.ylabel('True'); plt.xlabel('Predicted'); plt.show()

print("\nClassification report (ensemble at 0.5):")
print(classification_report(y_all, ensemble_preds, target_names=LABELS, digits=4))

plt.figure(figsize=(12,6))
for i in range(8):
    plt.subplot(2,4,i+1)
    plt.imshow(imgs_all[i].squeeze(), cmap='gray')
    true_lbl = LABELS[y_all[i]]
    pred_lbl = LABELS[ensemble_preds[i]]
    plt.title(f"T:{true_lbl}\nP:{pred_lbl}\nProb:{ensemble_probs[i]:.2f}")
    plt.axis('off')
plt.suptitle('Sample Ensemble Predictions'); plt.show()

os.makedirs("outputs", exist_ok=True)
for name, model in models.items():
    fname = f"outputs/{name}.weights.h5"
    model.save_weights(fname)
print("Saved weights to ./outputs/")

print("\nSUMMARY of accuracies:")
for name, acc in accs.items():
    print(f"  {name}: {acc:.4f}")
print(f"  Ensemble: {ensemble_acc:.4f}, Ensemble AUC: {roc_auc:.4f}")

import os, random, json
import numpy as np
import matplotlib.pyplot as plt
import tensorflow as tf
import tensorflow_datasets as tfds
from sklearn.metrics import (
    accuracy_score, confusion_matrix, classification_report,
    roc_curve, auc, precision_recall_curve, brier_score_loss
)
from sklearn.calibration import calibration_curve
import pandas as pd

SEED = 123
np.random.seed(SEED)
tf.random.set_seed(SEED)
random.seed(SEED)

(ds_train, ds_test), ds_info = tfds.load(
    "pneumonia_mnist",
    split=["train", "test"],
    as_supervised=True,
    with_info=True,
)
LABELS = ds_info.features['label'].names
IMG_SIZE = (64, 64)
BATCH = 32

def preprocess(image, label):
    image = tf.image.resize(image, IMG_SIZE) / 255.0
    image = tf.reshape(image, [IMG_SIZE[0], IMG_SIZE[1], 1])
    return image, label

train_ds = ds_train.map(preprocess).shuffle(512, seed=SEED).batch(BATCH).prefetch(tf.data.AUTOTUNE)
test_ds  = ds_test.map(preprocess).batch(BATCH).prefetch(tf.data.AUTOTUNE)


imgs_all, y_all = [], []
for x, y in test_ds:
    imgs_all.append(x.numpy()); y_all.append(y.numpy())
imgs_all = np.concatenate(imgs_all, axis=0)
y_all    = np.concatenate(y_all, axis=0)
N = len(y_all)
print("Test size:", N, "| Labels:", LABELS)

def build_simple_cnn():
    inp = tf.keras.Input(shape=(IMG_SIZE[0], IMG_SIZE[1], 1))
    x = tf.keras.layers.Conv2D(32, 3, activation='relu', padding='same')(inp)
    x = tf.keras.layers.MaxPooling2D()(x)
    x = tf.keras.layers.Conv2D(64, 3, activation='relu', padding='same')(x)
    x = tf.keras.layers.MaxPooling2D()(x)
    x = tf.keras.layers.Flatten()(x)
    x = tf.keras.layers.Dense(128, activation='relu')(x)
    x = tf.keras.layers.Dropout(0.4)(x)
    out = tf.keras.layers.Dense(1, activation='sigmoid')(x)
    m = tf.keras.Model(inp, out)
    m.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
    return m

def resnet_block(x, f=32):
    y = tf.keras.layers.Conv2D(f, 3, padding='same', activation='relu')(x)
    y = tf.keras.layers.Conv2D(f, 3, padding='same')(y)
    y = tf.keras.layers.Add()([x, y])
    return tf.keras.layers.Activation('relu')(y)

def build_resnet_like():
    inp = tf.keras.Input(shape=(IMG_SIZE[0], IMG_SIZE[1], 1))
    x = tf.keras.layers.Conv2D(32, 3, padding='same', activation='relu')(inp)
    x = tf.keras.layers.MaxPooling2D()(x)
    x = resnet_block(x, 32)
    x = tf.keras.layers.MaxPooling2D()(x)
    x = tf.keras.layers.Flatten()(x)
    x = tf.keras.layers.Dense(64, activation='relu')(x)
    x = tf.keras.layers.Dropout(0.4)(x)
    out = tf.keras.layers.Dense(1, activation='sigmoid')(x)
    m = tf.keras.Model(inp, out)
    m.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
    return m

def dense_block(x, f=16):
    y = tf.keras.layers.Conv2D(f, 3, padding='same', activation='relu')(x)
    return tf.keras.layers.Concatenate()([x, y])

def build_densenet_like():
    inp = tf.keras.Input(shape=(IMG_SIZE[0], IMG_SIZE[1], 1))
    x = tf.keras.layers.Conv2D(16, 3, padding='same', activation='relu')(inp)
    x = dense_block(x, 16)
    x = tf.keras.layers.MaxPooling2D()(x)
    x = dense_block(x, 32)
    x = tf.keras.layers.GlobalAveragePooling2D()(x)
    x = tf.keras.layers.Dense(64, activation='relu')(x)
    x = tf.keras.layers.Dropout(0.4)(x)
    out = tf.keras.layers.Dense(1, activation='sigmoid')(x)
    m = tf.keras.Model(inp, out)
    m.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
    return m

def build_vgg_like():
    inp = tf.keras.Input(shape=(IMG_SIZE[0], IMG_SIZE[1], 1))
    x = tf.keras.layers.Conv2D(32, 3, activation='relu', padding='same')(inp)
    x = tf.keras.layers.Conv2D(32, 3, activation='relu', padding='same')(x)
    x = tf.keras.layers.MaxPooling2D()(x)
    x = tf.keras.layers.Conv2D(64, 3, activation='relu', padding='same')(x)
    x = tf.keras.layers.Conv2D(64, 3, activation='relu', padding='same')(x)
    x = tf.keras.layers.MaxPooling2D()(x)
    x = tf.keras.layers.Flatten()(x)
    x = tf.keras.layers.Dense(128, activation='relu')(x)
    x = tf.keras.layers.Dropout(0.4)(x)
    out = tf.keras.layers.Dense(1, activation='sigmoid')(x)
    m = tf.keras.Model(inp, out)
    m.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
    return m

def inception_module(x, f=16):
    p1 = tf.keras.layers.Conv2D(f, 1, padding='same', activation='relu')(x)
    p2 = tf.keras.layers.Conv2D(f, 3, padding='same', activation='relu')(x)
    p3 = tf.keras.layers.Conv2D(f, 5, padding='same', activation='relu')(x)
    p4 = tf.keras.layers.MaxPooling2D(3, strides=1, padding='same')(x)
    p4 = tf.keras.layers.Conv2D(f, 1, padding='same', activation='relu')(p4)
    return tf.keras.layers.Concatenate()([p1, p2, p3, p4])

def build_inception_like():
    inp = tf.keras.Input(shape=(IMG_SIZE[0], IMG_SIZE[1], 1))
    x = tf.keras.layers.Conv2D(16, 3, padding='same', activation='relu')(inp)
    x = inception_module(x, 16)
    x = tf.keras.layers.MaxPooling2D()(x)
    x = inception_module(x, 32)
    x = tf.keras.layers.GlobalAveragePooling2D()(x)
    x = tf.keras.layers.Dense(64, activation='relu')(x)
    x = tf.keras.layers.Dropout(0.4)(x)
    out = tf.keras.layers.Dense(1, activation='sigmoid')(x)
    m = tf.keras.Model(inp, out)
    m.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
    return m

models = {
    "CNN": build_simple_cnn(),
    "ResNet": build_resnet_like(),
    "DenseNet": build_densenet_like(),
    "VGG": build_vgg_like(),
    "Inception": build_inception_like(),
}

EPOCHS = 2
histories = {}
for name, model in models.items():
    print(f"\nTraining {name}...")
    histories[name] = model.fit(train_ds, validation_data=test_ds, epochs=EPOCHS, verbose=1)

probs_dict, accs = {}, {}
for name, model in models.items():
    probs = model.predict(imgs_all, batch_size=64, verbose=0).ravel()
    probs_dict[name] = probs
    preds = (probs >= 0.5).astype(int)
    accs[name] = accuracy_score(y_all, preds)
    print(f"{name} accuracy (thr=0.5): {accs[name]:.4f}")


ensemble_probs = np.mean(np.stack(list(probs_dict.values()), axis=0), axis=0)
ensemble_preds = (ensemble_probs >= 0.5).astype(int)
ensemble_acc = accuracy_score(y_all, ensemble_preds)
print(f"\nEnsemble accuracy (thr=0.5): {ensemble_acc:.4f}")

out_dir = "outputs"
os.makedirs(out_dir, exist_ok=True)

rows = []
for name in models.keys():
    p = probs_dict[name]
    preds = (p >= 0.5).astype(int)
    cm  = confusion_matrix(y_all, preds)

    rows.append({
        "model": name,
        "accuracy": accuracy_score(y_all, preds),
        "tn": cm[0,0], "fp": cm[0,1], "fn": cm[1,0], "tp": cm[1,1],
    })


cm_ens = confusion_matrix(y_all, ensemble_preds)
rows.append({
    "model": "Ensemble",
    "accuracy": ensemble_acc,
    "tn": cm_ens[0,0], "fp": cm_ens[0,1], "fn": cm_ens[1,0], "tp": cm_ens[1,1],
})

df_metrics = pd.DataFrame(rows)
print("\n=== OUTPUT 1: Metrics Table ===")
print(df_metrics.to_string(index=False))
csv_path = os.path.join(out_dir, "metrics_table.csv")
df_metrics.to_csv(csv_path, index=False)
print("Saved:", csv_path)

fpr, tpr, _ = roc_curve(y_all, ensemble_probs)
roc_auc = auc(fpr, tpr)
print(f"\n=== OUTPUT 2: ROC AUC (Ensemble) = {roc_auc:.4f}")

plt.figure(figsize=(6,5))
plt.plot(fpr, tpr, label=f"AUC = {roc_auc:.3f}")
plt.plot([0,1],[0,1],'--', color='gray')
plt.xlabel("False Positive Rate")
plt.ylabel("True Positive Rate")
plt.title("Ensemble ROC")
plt.legend(); plt.grid(True)
plt.show()

prec, rec, _ = precision_recall_curve(y_all, ensemble_probs)
pr_auc = auc(rec, prec)
print(f"\n=== OUTPUT 3: PR AUC (Ensemble) = {pr_auc:.4f}")

plt.figure(figsize=(6,5))
plt.plot(rec, prec, label=f"PR AUC = {pr_auc:.3f}")
plt.xlabel("Recall")
plt.ylabel("Precision")
plt.title("Ensemble Precision–Recall")
plt.legend(); plt.grid(True)
plt.show()

prob_true, prob_pred = calibration_curve(y_all, ensemble_probs, n_bins=10)
brier = brier_score_loss(y_all, ensemble_probs)
print(f"\n=== OUTPUT 4: Brier score (Ensemble) = {brier:.4f} (lower is better)")

plt.figure(figsize=(6,5))
plt.plot(prob_pred, prob_true, marker='o', label='Reliability')
plt.plot([0,1],[0,1],'--', label='Perfect')
plt.xlabel("Mean predicted probability")
plt.ylabel("Fraction of positives")
plt.title("Calibration (Reliability) Curve — Ensemble")
plt.legend(); plt.grid(True)
plt.show()

fn_idx = np.where((y_all == 1) & (ensemble_preds == 0))[0]
print(f"\n=== OUTPUT 5: False Negatives — total {len(fn_idx)} ===")


if len(fn_idx) > 0:
    order = np.argsort(ensemble_probs[fn_idx])
    top_k = fn_idx[order][:8]

    err_dir = os.path.join(out_dir, "false_negatives")
    os.makedirs(err_dir, exist_ok=True)

    plt.figure(figsize=(12,6))
    for i, idx in enumerate(top_k):
        plt.subplot(2,4,i+1)
        plt.imshow(imgs_all[idx].squeeze(), cmap='gray')
        plt.title(f"Idx:{idx}\nProb:{ensemble_probs[idx]:.2f}")
        plt.axis('off')

        fname = os.path.join(err_dir, f"FN_idx{idx}_prob{ensemble_probs[idx]:.3f}.png")
        plt.imsave(fname, imgs_all[idx].squeeze(), cmap='gray')
    plt.suptitle("Top False Negatives (Ensemble)")
    plt.show()
    print("Saved FN PNGs to:", err_dir)
else:
    print("No false negatives to display/save.")

for name, model in models.items():
    model.save_weights(os.path.join(out_dir, f"{name}.weights.h5"))
print("Saved weights to:", out_dir)

print("\nSUMMARY:")
print(df_metrics.to_string(index=False))
print(f"Ensemble ROC AUC: {roc_auc:.4f} | PR AUC: {pr_auc:.4f} | Brier: {brier:.4f}")

import os, random
import numpy as np
import matplotlib.pyplot as plt
import tensorflow as tf
import tensorflow_datasets as tfds
from sklearn.metrics import confusion_matrix, roc_curve, auc, accuracy_score
from sklearn.manifold import TSNE

SEED = 42
np.random.seed(SEED)
tf.random.set_seed(SEED)
random.seed(SEED)

OUT = "outputs_diff"
os.makedirs(OUT, exist_ok=True)

(ds_train, ds_test), ds_info = tfds.load(
    "pneumonia_mnist",
    split=["train", "test"],
    as_supervised=True,
    with_info=True,
)
LABELS = ds_info.features["label"].names
IMG_SIZE = (64,64)
BATCH = 32

def preprocess(img, lbl):
    img = tf.image.resize(img, IMG_SIZE) / 255.0
    img = tf.reshape(img, [IMG_SIZE[0], IMG_SIZE[1], 1])
    return img, lbl

train_ds = ds_train.map(preprocess).shuffle(512, seed=SEED).batch(BATCH).prefetch(tf.data.AUTOTUNE)
test_ds  = ds_test.map(preprocess).batch(BATCH).prefetch(tf.data.AUTOTUNE)


imgs_all_list, y_all_list = [], []
for imgs, labs in test_ds:
    imgs_all_list.append(imgs.numpy())
    y_all_list.append(labs.numpy())
imgs_all = np.concatenate(imgs_all_list, axis=0)
y_all = np.concatenate(y_all_list, axis=0)
N = len(y_all)
print("Test size:", N, "| Labels:", LABELS)

def build_simple_cnn(name="SimpleCNN"):
    inp = tf.keras.Input(shape=(IMG_SIZE[0], IMG_SIZE[1], 1))
    x = tf.keras.layers.Conv2D(32,3,activation='relu',padding='same')(inp)
    x = tf.keras.layers.MaxPooling2D()(x)
    x = tf.keras.layers.Conv2D(64,3,activation='relu',padding='same')(x)
    x = tf.keras.layers.MaxPooling2D()(x)
    x = tf.keras.layers.Conv2D(128,3,activation='relu',padding='same', name='last_conv')(x)
    x = tf.keras.layers.GlobalAveragePooling2D()(x)
    pen = tf.keras.layers.Dense(64, activation='relu', name='penultimate')(x)
    out = tf.keras.layers.Dense(1, activation='sigmoid', name='output')(pen)
    m = tf.keras.Model(inp, out, name=name)
    m.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
    return m

def resnet_block(x, filters):
    y = tf.keras.layers.Conv2D(filters,3, padding='same', activation='relu')(x)
    y = tf.keras.layers.Conv2D(filters,3, padding='same')(y)
    out = tf.keras.layers.Add()([x, y])
    return tf.keras.layers.Activation('relu')(out)

def build_resnet_like():
    inp = tf.keras.Input(shape=(IMG_SIZE[0], IMG_SIZE[1], 1))
    x = tf.keras.layers.Conv2D(32,3,activation='relu',padding='same')(inp)
    x = tf.keras.layers.MaxPooling2D()(x)
    x = resnet_block(x,32)
    x = tf.keras.layers.GlobalAveragePooling2D()(x)
    pen = tf.keras.layers.Dense(64,activation='relu', name='penultimate')(x)
    out = tf.keras.layers.Dense(1, activation='sigmoid', name='output')(pen)
    m = tf.keras.Model(inp, out, name='ResNetLike')
    m.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
    return m

def dense_block(x, filters):
    y = tf.keras.layers.Conv2D(filters,3, padding='same', activation='relu')(x)
    return tf.keras.layers.Concatenate()([x, y])

def build_densenet_like():
    inp = tf.keras.Input(shape=(IMG_SIZE[0], IMG_SIZE[1], 1))
    x = tf.keras.layers.Conv2D(16,3,activation='relu',padding='same')(inp)
    x = dense_block(x,16)
    x = tf.keras.layers.MaxPooling2D()(x)
    x = dense_block(x,32)
    x = tf.keras.layers.GlobalAveragePooling2D()(x)
    pen = tf.keras.layers.Dense(64, activation='relu', name='penultimate')(x)
    out = tf.keras.layers.Dense(1, activation='sigmoid', name='output')(pen)
    m = tf.keras.Model(inp,out, name='DenseNetLike')
    m.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
    return m

def build_vgg_like():
    inp = tf.keras.Input(shape=(IMG_SIZE[0], IMG_SIZE[1], 1))
    x = tf.keras.layers.Conv2D(32,3,activation='relu',padding='same')(inp)
    x = tf.keras.layers.Conv2D(32,3,activation='relu',padding='same')(x)
    x = tf.keras.layers.MaxPooling2D()(x)
    x = tf.keras.layers.Conv2D(64,3,activation='relu',padding='same')(x)
    x = tf.keras.layers.Conv2D(64,3,activation='relu',padding='same', name='last_conv_vgg')(x)
    x = tf.keras.layers.GlobalAveragePooling2D()(x)
    pen = tf.keras.layers.Dense(64, activation='relu', name='penultimate')(x)
    out = tf.keras.layers.Dense(1, activation='sigmoid', name='output')(pen)
    m = tf.keras.Model(inp,out, name='VGGLike')
    m.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
    return m

def inception_module(x, f=16):
    p1 = tf.keras.layers.Conv2D(f,1,activation='relu',padding='same')(x)
    p2 = tf.keras.layers.Conv2D(f,3,activation='relu',padding='same')(x)
    p3 = tf.keras.layers.Conv2D(f,5,activation='relu',padding='same')(x)
    p4 = tf.keras.layers.MaxPooling2D(3, strides=1, padding='same')(x)
    p4 = tf.keras.layers.Conv2D(f,1,activation='relu',padding='same')(p4)
    return tf.keras.layers.Concatenate()([p1,p2,p3,p4])

def build_inception_like():
    inp = tf.keras.Input(shape=(IMG_SIZE[0], IMG_SIZE[1], 1))
    x = tf.keras.layers.Conv2D(16,3,activation='relu',padding='same')(inp)
    x = inception_module(x, 16)
    x = tf.keras.layers.GlobalAveragePooling2D()(x)
    pen = tf.keras.layers.Dense(64, activation='relu', name='penultimate')(x)
    out = tf.keras.layers.Dense(1, activation='sigmoid', name='output')(pen)
    m = tf.keras.Model(inp,out,name='InceptionLike')
    m.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
    return m


models = {
    "SimpleCNN": build_simple_cnn(),
    "ResNet": build_resnet_like(),
    "DenseNet": build_densenet_like(),
    "VGG": build_vgg_like(),
    "Inception": build_inception_like()
}

EPOCHS = 2
histories = {}
for name, model in models.items():
    print(f"\nTraining {name} ...")
    histories[name] = model.fit(train_ds, validation_data=test_ds, epochs=EPOCHS, verbose=1)

probs = {}
aucs = {}
accs = {}
for name, model in models.items():
    p = model.predict(imgs_all, batch_size=64, verbose=0).ravel()
    probs[name] = p
    preds = (p >= 0.5).astype(int)
    accs[name] = accuracy_score(y_all, preds)
    cm = confusion_matrix(y_all, preds)

    plt.figure(figsize=(4,3))
    plt.imshow(cm, cmap='Blues', interpolation='nearest')
    plt.title(f"{name} Confusion Matrix")
    plt.xticks([0,1], LABELS, rotation=45); plt.yticks([0,1], LABELS)
    th = cm.max() / 2.
    for i in range(cm.shape[0]):
        for j in range(cm.shape[1]):
            plt.text(j, i, int(cm[i,j]), ha='center', color='white' if cm[i,j] > th else 'black')
    plt.tight_layout()
    cm_path = os.path.join(OUT, f"{name}_confusion.png")
    plt.savefig(cm_path)
    plt.close()

    fpr, tpr, _ = roc_curve(y_all, p)
    aucs[name] = auc(fpr, tpr)
    print(f"{name}: acc={accs[name]:.4f}, AUC={aucs[name]:.4f} (confusion saved -> {cm_path})")

names = list(aucs.keys())
vals = [aucs[n] for n in names]
plt.figure(figsize=(8,4))
bars = plt.bar(names, vals)
plt.ylim(0.0, 1.0)
plt.ylabel("ROC AUC")
plt.title("Per-model ROC AUC comparison")
for bar, v in zip(bars, vals):
    plt.text(bar.get_x() + bar.get_width()/2, v + 0.01, f"{v:.3f}", ha='center')
auc_path = os.path.join(OUT, "per_model_auc.png")
plt.savefig(auc_path); plt.show()
print("Saved AUC bar chart ->", auc_path)

ensemble_probs = np.mean(np.stack(list(probs.values()), axis=0), axis=0)
plt.figure(figsize=(6,4))
plt.hist(ensemble_probs, bins=20, edgecolor='k')
plt.title("Ensemble predicted probability histogram (confidence)")
plt.xlabel("Predicted probability for PNEUMONIA")
plt.ylabel("Count")
hist_path = os.path.join(OUT, "ensemble_confidence_hist.png")
plt.savefig(hist_path); plt.show()
print("Saved ensemble confidence histogram ->", hist_path)

def find_last_conv_layer(model):
    for layer in reversed(model.layers):
        if isinstance(layer, tf.keras.layers.Conv2D):
            return layer.name
    return None

def make_gradcam(model, img, last_conv_name=None):

    img_batch = np.expand_dims(img, axis=0).astype(np.float32)
    if last_conv_name is None:
        last_conv_name = find_last_conv_layer(model)
    grad_model = tf.keras.models.Model([model.inputs], [model.get_layer(last_conv_name).output, model.output])
    with tf.GradientTape() as tape:
        conv_outputs, preds = grad_model(img_batch)
        loss = preds[:, 0]
    grads = tape.gradient(loss, conv_outputs)
    pooled_grads = tf.reduce_mean(grads, axis=(0,1,2))
    conv_outputs = conv_outputs[0].numpy()
    pooled_grads = pooled_grads.numpy()
    for i in range(pooled_grads.shape[-1]):
        conv_outputs[..., i] *= pooled_grads[i]
    heatmap = np.mean(conv_outputs, axis=-1)
    heatmap = np.maximum(heatmap, 0)
    heatmap = heatmap / (np.max(heatmap) + 1e-8)
    heatmap_resized = tf.image.resize(heatmap[..., np.newaxis], IMG_SIZE).numpy().squeeze()
    return heatmap_resized


cam_model = models["SimpleCNN"]
last_conv = find_last_conv_layer(cam_model)
print("Using last conv layer for Grad-CAM:", last_conv)


ens_preds = (ensemble_probs >= 0.5).astype(int)
tp_idx = np.where((y_all == 1) & (ens_preds == 1))[0]
fn_idx = np.where((y_all == 1) & (ens_preds == 0))[0]

selected = []
if len(tp_idx) >= 2:
    selected.extend(tp_idx[:2])
if len(fn_idx) >= 2:
    selected.extend(fn_idx[:2])

if len(selected) < 4:
    selected = list(np.random.choice(np.arange(N), size=4, replace=False))

print("Grad-CAM selected indices:", selected)

gradcam_dir = os.path.join(OUT, "gradcam")
os.makedirs(gradcam_dir, exist_ok=True)
for idx in selected:
    img = imgs_all[idx].squeeze()
    heat = make_gradcam(cam_model, imgs_all[idx], last_conv_name=last_conv)

    cmap = plt.get_cmap("jet")
    heat_col = cmap(heat)[:,:,:3]
    img_rgb = np.repeat(img[..., np.newaxis], 3, axis=-1)
    overlay = 0.5 * img_rgb + 0.5 * heat_col
    overlay = np.clip(overlay, 0, 1)
    plt.figure(figsize=(6,3))
    plt.subplot(1,2,1)
    plt.imshow(img, cmap='gray'); plt.title(f"Idx {idx} True:{LABELS[y_all[idx]]}")
    plt.axis('off')
    plt.subplot(1,2,2)
    plt.imshow(overlay); plt.title("Grad-CAM overlay")
    plt.axis('off')
    fname = os.path.join(gradcam_dir, f"gradcam_idx{idx}.png")
    plt.savefig(fname); plt.close()
    print("Saved Grad-CAM ->", fname)

def extract_penultimate(model, imgs, layer_name='penultimate'):
    inter = tf.keras.Model(inputs=model.input, outputs=model.get_layer(layer_name).output)
    feats = inter.predict(imgs, batch_size=64, verbose=0)
    return feats


max_samples = min(300, N)
sel_idx = np.random.choice(np.arange(N), size=max_samples, replace=False)
sel_imgs = imgs_all[sel_idx]
sel_labels = y_all[sel_idx]

feats_simple = extract_penultimate(models["SimpleCNN"], sel_imgs)
feats_vgg = extract_penultimate(models["VGG"], sel_imgs)

feats_concat = np.concatenate([feats_simple, feats_vgg], axis=1)

print("Running t-SNE on", feats_concat.shape[0], "samples ... (this may take a few seconds)")
tsne = TSNE(n_components=2, random_state=SEED, init='pca', learning_rate='auto', perplexity=30)
feat2d = tsne.fit_transform(feats_concat)

plt.figure(figsize=(7,6))
colors = ['tab:blue' if l==0 else 'tab:orange' for l in sel_labels]
plt.scatter(feat2d[:,0], feat2d[:,1], c=colors, s=10, alpha=0.8)
plt.title("t-SNE of penultimate features (SimpleCNN + VGG concatenated)")
plt.xlabel("TSNE-1"); plt.ylabel("TSNE-2")
tsne_path = os.path.join(OUT, "tsne_penultimate.png")
plt.savefig(tsne_path); plt.show()
print("Saved t-SNE plot ->", tsne_path)

for name, model in models.items():
    model.save_weights(os.path.join(OUT, f"{name}.weights.h5"))

print("\nAll outputs saved under directory:", OUT)
print("Per-model confusion PNGs, AUC bar chart, ensemble histogram, Grad-CAM PNGs, t-SNE PNG.")

import os, random, json
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
import tensorflow as tf
import tensorflow_datasets as tfds
from sklearn.metrics import precision_score, recall_score, accuracy_score, confusion_matrix

SEED = 42
np.random.seed(SEED)
tf.random.set_seed(SEED)
random.seed(SEED)

OUTDIR = "outputs_more"
os.makedirs(OUTDIR, exist_ok=True)

(ds_train, ds_test), ds_info = tfds.load(
    "pneumonia_mnist",
    split=["train", "test"],
    as_supervised=True,
    with_info=True,
)
LABELS = ds_info.features['label'].names  # ['NORMAL','PNEUMONIA']
print("Labels:", LABELS)

IMG_SIZE = (64,64)
BATCH = 32

def preprocess(img, lbl):
    img = tf.image.resize(img, IMG_SIZE) / 255.0
    img = tf.reshape(img, [IMG_SIZE[0], IMG_SIZE[1], 1])
    return img, lbl

train_ds = ds_train.map(preprocess).shuffle(512, seed=SEED).batch(BATCH).prefetch(tf.data.AUTOTUNE)
test_ds  = ds_test.map(preprocess).batch(BATCH).prefetch(tf.data.AUTOTUNE)


imgs_list, y_list = [], []
for x, y in test_ds:
    imgs_list.append(x.numpy())
    y_list.append(y.numpy())
imgs_all = np.concatenate(imgs_list, axis=0)
y_all = np.concatenate(y_list, axis=0)
N = len(y_all)
print("Test set size:", N)

def build_simple_cnn():
    inp = tf.keras.Input(shape=(IMG_SIZE[0], IMG_SIZE[1], 1))
    x = tf.keras.layers.Conv2D(32,3,activation='relu',padding='same')(inp)
    x = tf.keras.layers.MaxPooling2D()(x)
    x = tf.keras.layers.Conv2D(64,3,activation='relu',padding='same')(x)
    x = tf.keras.layers.MaxPooling2D()(x)
    x = tf.keras.layers.Flatten()(x)
    x = tf.keras.layers.Dense(128,activation='relu')(x)
    x = tf.keras.layers.Dropout(0.4)(x)
    out = tf.keras.layers.Dense(1,activation='sigmoid')(x)
    m = tf.keras.Model(inp,out)
    m.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
    return m

def resnet_block(x, f=32):
    y = tf.keras.layers.Conv2D(f,3,padding='same',activation='relu')(x)
    y = tf.keras.layers.Conv2D(f,3,padding='same')(y)
    y = tf.keras.layers.Add()([x,y])
    return tf.keras.layers.Activation('relu')(y)

def build_resnet_like():
    inp = tf.keras.Input(shape=(IMG_SIZE[0], IMG_SIZE[1], 1))
    x = tf.keras.layers.Conv2D(32,3,activation='relu',padding='same')(inp)
    x = tf.keras.layers.MaxPooling2D()(x)
    x = resnet_block(x,32)
    x = tf.keras.layers.GlobalAveragePooling2D()(x)
    x = tf.keras.layers.Dense(64,activation='relu')(x)
    x = tf.keras.layers.Dropout(0.4)(x)
    out = tf.keras.layers.Dense(1,activation='sigmoid')(x)
    m = tf.keras.Model(inp,out)
    m.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
    return m

def dense_block(x, f=16):
    y = tf.keras.layers.Conv2D(f,3,padding='same',activation='relu')(x)
    return tf.keras.layers.Concatenate()([x,y])

def build_densenet_like():
    inp = tf.keras.Input(shape=(IMG_SIZE[0], IMG_SIZE[1], 1))
    x = tf.keras.layers.Conv2D(16,3,activation='relu',padding='same')(inp)
    x = dense_block(x,16)
    x = tf.keras.layers.MaxPooling2D()(x)
    x = dense_block(x,32)
    x = tf.keras.layers.GlobalAveragePooling2D()(x)
    x = tf.keras.layers.Dense(64,activation='relu')(x)
    x = tf.keras.layers.Dropout(0.4)(x)
    out = tf.keras.layers.Dense(1,activation='sigmoid')(x)
    m = tf.keras.Model(inp,out)
    m.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
    return m

def build_vgg_like():
    inp = tf.keras.Input(shape=(IMG_SIZE[0], IMG_SIZE[1], 1))
    x = tf.keras.layers.Conv2D(32,3,activation='relu',padding='same')(inp)
    x = tf.keras.layers.Conv2D(32,3,activation='relu',padding='same')(x)
    x = tf.keras.layers.MaxPooling2D()(x)
    x = tf.keras.layers.Conv2D(64,3,activation='relu',padding='same')(x)
    x = tf.keras.layers.Conv2D(64,3,activation='relu',padding='same')(x)
    x = tf.keras.layers.MaxPooling2D()(x)
    x = tf.keras.layers.Flatten()(x)
    x = tf.keras.layers.Dense(128,activation='relu')(x)
    x = tf.keras.layers.Dropout(0.4)(x)
    out = tf.keras.layers.Dense(1,activation='sigmoid')(x)
    m = tf.keras.Model(inp,out)
    m.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
    return m

def inception_module(x, f=16):
    p1 = tf.keras.layers.Conv2D(f,1,activation='relu',padding='same')(x)
    p2 = tf.keras.layers.Conv2D(f,3,activation='relu',padding='same')(x)
    p3 = tf.keras.layers.Conv2D(f,5,activation='relu',padding='same')(x)
    p4 = tf.keras.layers.MaxPooling2D(3, strides=1, padding='same')(x)
    p4 = tf.keras.layers.Conv2D(f,1,activation='relu',padding='same')(p4)
    return tf.keras.layers.Concatenate()([p1,p2,p3,p4])

def build_inception_like():
    inp = tf.keras.Input(shape=(IMG_SIZE[0], IMG_SIZE[1], 1))
    x = tf.keras.layers.Conv2D(16,3,activation='relu',padding='same')(inp)
    x = inception_module(x,16)
    x = tf.keras.layers.GlobalAveragePooling2D()(x)
    x = tf.keras.layers.Dense(64,activation='relu')(x)
    x = tf.keras.layers.Dropout(0.4)(x)
    out = tf.keras.layers.Dense(1,activation='sigmoid')(x)
    m = tf.keras.Model(inp,out)
    m.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
    return m

models = {
    "CNN": build_simple_cnn(),
    "ResNet": build_resnet_like(),
    "DenseNet": build_densenet_like(),
    "VGG": build_vgg_like(),
    "Inception": build_inception_like()
}

EPOCHS = 2
histories = {}
for name, model in models.items():
    print(f"\nTraining {name} ...")
    histories[name] = model.fit(train_ds, validation_data=test_ds, epochs=EPOCHS, verbose=1)

probs_dict = {}
metrics_rows = []
for name, model in models.items():
    probs = model.predict(imgs_all, batch_size=64, verbose=0).ravel()
    preds = (probs >= 0.5).astype(int)
    probs_dict[name] = probs
    acc = accuracy_score(y_all, preds)
    prec = precision_score(y_all, preds, zero_division=0)
    rec = recall_score(y_all, preds, zero_division=0)
    tn, fp, fn, tp = confusion_matrix(y_all, preds).ravel()
    metrics_rows.append({
        "model": name,
        "accuracy": acc,
        "precision": prec,
        "recall": rec,
        "tn": int(tn), "fp": int(fp), "fn": int(fn), "tp": int(tp),
        "params": int(model.count_params())
    })
    print(f"{name}: acc={acc:.4f}, prec={prec:.4f}, rec={rec:.4f}, params={model.count_params()}")


ensemble_probs = np.mean(np.stack(list(probs_dict.values()), axis=0), axis=0)
ensemble_preds = (ensemble_probs >= 0.5).astype(int)
ensemble_acc = accuracy_score(y_all, ensemble_preds)
ensemble_prec = precision_score(y_all, ensemble_preds, zero_division=0)
ensemble_rec = recall_score(y_all, ensemble_preds, zero_division=0)

metrics_rows.append({
    "model": "Ensemble",
    "accuracy": ensemble_acc,
    "precision": ensemble_prec,
    "recall": ensemble_rec,
    "tn": int(confusion_matrix(y_all, ensemble_preds).ravel()[0]),
    "fp": int(confusion_matrix(y_all, ensemble_preds).ravel()[1]),
    "fn": int(confusion_matrix(y_all, ensemble_preds).ravel()[2]),
    "tp": int(confusion_matrix(y_all, ensemble_preds).ravel()[3]),
    "params": sum([int(m.count_params()) for m in models.values()])
})


df_summary = pd.DataFrame(metrics_rows)
summary_path = os.path.join(OUTDIR, "model_summary.csv")
df_summary.to_csv(summary_path, index=False)
print("\n=== OUTPUT 5: Model summary saved ->", summary_path)
print(df_summary.to_string(index=False))

for name, hist in histories.items():
    plt.figure(figsize=(10,4))

    plt.subplot(1,2,1)
    plt.plot(hist.history['loss'], label='train loss')
    if 'val_loss' in hist.history:
        plt.plot(hist.history['val_loss'], label='val loss')
    plt.title(f"{name} Loss")
    plt.xlabel("Epoch"); plt.ylabel("Loss"); plt.legend()

    plt.subplot(1,2,2)
    plt.plot(hist.history.get('accuracy', []), label='train acc')
    if 'val_accuracy' in hist.history:
        plt.plot(hist.history['val_accuracy'], label='val acc')
    plt.title(f"{name} Accuracy")
    plt.xlabel("Epoch"); plt.ylabel("Accuracy"); plt.legend()
    fname = os.path.join(OUTDIR, f"{name}_learning_curves.png")
    plt.tight_layout(); plt.savefig(fname); plt.close()
    print("Saved learning curves ->", fname)

models_list = [r['model'] for r in metrics_rows]
prec_vals = [r['precision'] for r in metrics_rows]
rec_vals  = [r['recall'] for r in metrics_rows]

plt.figure(figsize=(8,5))
x = np.arange(len(models_list))
width = 0.35
plt.bar(x - width/2, prec_vals, width, label='Precision')
plt.bar(x + width/2, rec_vals, width, label='Recall')
plt.xticks(x, models_list, rotation=45)
plt.ylabel("Score"); plt.ylim(0,1)
plt.title("Per-model Precision & Recall (incl. Ensemble)")
plt.legend()
precrec_path = os.path.join(OUTDIR, "precision_recall_bar.png")
plt.tight_layout(); plt.savefig(precrec_path); plt.show()
print("=== OUTPUT 2: Saved precision/recall bar chart ->", precrec_path)

df_preds = pd.DataFrame({
    "true_label": y_all
})
for name, probs in probs_dict.items():
    df_preds[f"prob_{name}"] = probs
df_preds["prob_ensemble"] = ensemble_probs
df_preds["pred_ensemble"] = ensemble_preds
preds_csv = os.path.join(OUTDIR, "per_sample_predictions.csv")
df_preds.to_csv(preds_csv, index=False)
print("=== OUTPUT 3: Saved per-sample predictions CSV ->", preds_csv)

tp_idx = np.where((y_all == 1) & (ensemble_preds == 1))[0]
print("Total true positives (ensemble):", len(tp_idx))
if len(tp_idx) > 0:
    sorted_tp = tp_idx[np.argsort(-ensemble_probs[tp_idx])]
    topk = sorted_tp[:12]
    gallery_dir = os.path.join(OUTDIR, "top_true_positives")
    os.makedirs(gallery_dir, exist_ok=True)
    plt.figure(figsize=(12,8))
    for i, idx in enumerate(topk):
        plt.subplot(3,4,i+1)
        plt.imshow(imgs_all[idx].squeeze(), cmap='gray')
        plt.title(f"Idx:{idx}\nProb:{ensemble_probs[idx]:.2f}")
        plt.axis('off')

        plt.imsave(os.path.join(gallery_dir, f"TP_idx{idx}_prob{ensemble_probs[idx]:.3f}.png"), imgs_all[idx].squeeze(), cmap='gray')
    plt.suptitle("Top True Positives (ensemble confidence desc.)")
    gallery_path = os.path.join(OUTDIR, "top_true_positives_grid.png")
    plt.tight_layout(); plt.savefig(gallery_path); plt.close()
    print("=== OUTPUT 4: Saved top true positives grid ->", gallery_path)
    print("saved individual TP PNGs ->", gallery_dir)
else:
    print("No true positives found to show.")

for name, model in models.items():
    model.save_weights(os.path.join(OUTDIR, f"{name}.weights.h5"))
print("Saved model weights (use .weights.h5) to", OUTDIR)

print("\nAll outputs saved under:", OUTDIR)
print("Files:")
for root, dirs, files in os.walk(OUTDIR):
    for f in files:
        print(" -", os.path.join(root, f))

import os, random, json
import numpy as np
import matplotlib.pyplot as plt
import tensorflow as tf
import tensorflow_datasets as tfds
from sklearn.metrics import (
    precision_recall_curve, auc, accuracy_score, confusion_matrix,
    matthews_corrcoef, balanced_accuracy_score, brier_score_loss
)
from sklearn.model_selection import train_test_split

SEED = 42
np.random.seed(SEED)
tf.random.set_seed(SEED)
random.seed(SEED)

OUT = "outputs_more_diff"
os.makedirs(OUT, exist_ok=True)

(ds_train_full, ds_test), ds_info = tfds.load(
    "pneumonia_mnist",
    split=["train", "test"],
    as_supervised=True,
    with_info=True,
)
LABELS = ds_info.features['label'].names
IMG_SIZE = (64, 64)
BATCH = 32

def preprocess(img, lbl):
    img = tf.image.resize(img, IMG_SIZE) / 255.0
    img = tf.reshape(img, [IMG_SIZE[0], IMG_SIZE[1], 1])
    return img, lbl


train_imgs_list, train_lbls_list = [], []
for x, y in ds_train_full.map(preprocess).batch(64).prefetch(tf.data.AUTOTUNE):
    train_imgs_list.append(x.numpy()); train_lbls_list.append(y.numpy())
train_imgs_all = np.concatenate(train_imgs_list, axis=0)
train_lbls_all = np.concatenate(train_lbls_list, axis=0)


train_X, val_X, train_y, val_y = train_test_split(
    train_imgs_all, train_lbls_all, test_size=0.1, random_state=SEED, stratify=train_lbls_all
)


train_ds = tf.data.Dataset.from_tensor_slices((train_X, train_y)).shuffle(512, seed=SEED).batch(BATCH).prefetch(tf.data.AUTOTUNE)

val_ds = tf.data.Dataset.from_tensor_slices((val_X, val_y)).batch(BATCH).prefetch(tf.data.AUTOTUNE)


test_ds_tf = ds_test.map(preprocess).batch(BATCH).prefetch(tf.data.AUTOTUNE)
imgs_all, y_all = [], []
for x,y in test_ds_tf:
    imgs_all.append(x.numpy()); y_all.append(y.numpy())
imgs_all = np.concatenate(imgs_all, axis=0)
y_all = np.concatenate(y_all, axis=0)
N = len(y_all)
print("Train size (approx):", len(train_X), "Val size:", len(val_X), "Test size:", N)

def build_simple_cnn():
    inp = tf.keras.Input(shape=(IMG_SIZE[0], IMG_SIZE[1], 1))
    x = tf.keras.layers.Conv2D(32,3,activation='relu',padding='same')(inp)
    x = tf.keras.layers.MaxPooling2D()(x)
    x = tf.keras.layers.Conv2D(64,3,activation='relu',padding='same')(x)
    x = tf.keras.layers.GlobalAveragePooling2D()(x)
    pen = tf.keras.layers.Dense(64, activation='relu', name='penultimate')(x)
    out = tf.keras.layers.Dense(1, activation='sigmoid', name='output')(pen)
    m = tf.keras.Model(inp, out)
    m.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
    return m

def resnet_block(x, filters=32):
    y = tf.keras.layers.Conv2D(filters,3, padding='same', activation='relu')(x)
    y = tf.keras.layers.Conv2D(filters,3, padding='same')(y)
    out = tf.keras.layers.Add()([x, y])
    return tf.keras.layers.Activation('relu')(out)

def build_resnet_like():
    inp = tf.keras.Input(shape=(IMG_SIZE[0], IMG_SIZE[1], 1))
    x = tf.keras.layers.Conv2D(32,3,activation='relu',padding='same')(inp)
    x = tf.keras.layers.MaxPooling2D()(x)
    x = resnet_block(x,32)
    x = tf.keras.layers.GlobalAveragePooling2D()(x)
    out = tf.keras.layers.Dense(1, activation='sigmoid')(x)
    m = tf.keras.Model(inp, out)
    m.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
    return m

def dense_block(x, f=16):
    y = tf.keras.layers.Conv2D(f,3, padding='same', activation='relu')(x)
    return tf.keras.layers.Concatenate()([x,y])

def build_densenet_like():
    inp = tf.keras.Input(shape=(IMG_SIZE[0], IMG_SIZE[1], 1))
    x = tf.keras.layers.Conv2D(16,3, activation='relu', padding='same')(inp)
    x = dense_block(x,16)
    x = tf.keras.layers.MaxPooling2D()(x)
    x = dense_block(x,32)
    x = tf.keras.layers.GlobalAveragePooling2D()(x)
    out = tf.keras.layers.Dense(1, activation='sigmoid')(x)
    m = tf.keras.Model(inp, out)
    m.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
    return m

def build_vgg_like():
    inp = tf.keras.Input(shape=(IMG_SIZE[0], IMG_SIZE[1], 1))
    x = tf.keras.layers.Conv2D(32,3,activation='relu',padding='same')(inp)
    x = tf.keras.layers.Conv2D(32,3,activation='relu',padding='same')(x)
    x = tf.keras.layers.MaxPooling2D()(x)
    x = tf.keras.layers.GlobalAveragePooling2D()(x)
    out = tf.keras.layers.Dense(1, activation='sigmoid')(x)
    m = tf.keras.Model(inp,out)
    m.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
    return m

def inception_module(x, f=16):
    p1 = tf.keras.layers.Conv2D(f,1,activation='relu',padding='same')(x)
    p2 = tf.keras.layers.Conv2D(f,3,activation='relu',padding='same')(x)
    p3 = tf.keras.layers.Conv2D(f,5,activation='relu',padding='same')(x)
    p4 = tf.keras.layers.MaxPooling2D(3, strides=1, padding='same')(x)
    p4 = tf.keras.layers.Conv2D(f,1,activation='relu',padding='same')(p4)
    return tf.keras.layers.Concatenate()([p1,p2,p3,p4])

def build_inception_like():
    inp = tf.keras.Input(shape=(IMG_SIZE[0], IMG_SIZE[1], 1))
    x = tf.keras.layers.Conv2D(16,3,activation='relu',padding='same')(inp)
    x = inception_module(x,16)
    x = tf.keras.layers.GlobalAveragePooling2D()(x)
    out = tf.keras.layers.Dense(1, activation='sigmoid')(x)
    m = tf.keras.Model(inp,out)
    m.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
    return m

models = {
    "CNN": build_simple_cnn(),
    "ResNet": build_resnet_like(),
    "DenseNet": build_densenet_like(),
    "VGG": build_vgg_like(),
    "Inception": build_inception_like()
}

EPOCHS = 2
histories = {}
for name, model in models.items():
    print(f"\nTraining {name} ...")
    histories[name] = model.fit(train_ds, validation_data=val_ds, epochs=EPOCHS, verbose=1)

metrics_list = []
probs_dict = {}
for name, model in models.items():
    p = model.predict(imgs_all, batch_size=64, verbose=0).ravel()
    probs_dict[name] = p
    preds = (p >= 0.5).astype(int)
    mcc = matthews_corrcoef(y_all, preds)
    bal = balanced_accuracy_score(y_all, preds)
    fpr, tpr, _ = precision_recall_curve(y_all, p)  # we will compute PR AUC below
    prec, rec, _ = precision_recall_curve(y_all, p)
    pr_auc = auc(rec, prec)
    metrics_list.append({
        "model": name,
        "mcc": float(mcc),
        "balanced_accuracy": float(bal),
        "pr_auc": float(pr_auc),
        "accuracy": float(accuracy_score(y_all, preds))
    })

    plt.figure(figsize=(5,4))
    plt.plot(rec, prec, label=f"PR AUC={pr_auc:.3f}")
    plt.xlabel("Recall"); plt.ylabel("Precision"); plt.title(f"{name} PR Curve")
    plt.legend(); plt.grid(True)
    plt.tight_layout()
    plt.savefig(os.path.join(OUT, f"{name}_pr_curve.png"))
    plt.close()
    print(f"{name}: MCC={mcc:.3f}, BalancedAcc={bal:.3f}, PR_AUC={pr_auc:.3f} (PR saved)")

df_metrics = tf.compat.v1.keras.utils.Progbar if False else None  # keep cell-level imports safe
import pandas as pd
df_metrics = pd.DataFrame(metrics_list)
csv_path = os.path.join(OUT, "per_model_mcc_balanced_pr.csv")
df_metrics.to_csv(csv_path, index=False)
print("\n=== OUTPUT 1 saved ->", csv_path)
print(df_metrics.to_string(index=False))

ensemble_probs = np.mean(np.stack(list(probs_dict.values()), axis=0), axis=0)
ensemble_preds = (ensemble_probs >= 0.5).astype(int)


def get_model_logits_on_set(model, imgs):

    p = model.predict(imgs, batch_size=64, verbose=0).ravel()

    p = np.clip(p, 1e-6, 1-1e-6)
    return np.log(p / (1 - p))


val_logits_stack = []
for name, model in models.items():
    val_logits_stack.append(get_model_logits_on_set(model, val_X))
val_logits_stack = np.stack(val_logits_stack, axis=0)
val_ens_logits = val_logits_stack.mean(axis=0)


def nll_T(T, logits, labels):
    scaled = logits / T
    p = 1 / (1 + np.exp(-scaled))

    eps = 1e-12
    return -np.mean(labels * np.log(p + eps) + (1-labels) * np.log(1 - p + eps))

Ts = np.linspace(0.1, 3.0, 60)
nlls = [nll_T(T, val_ens_logits, val_y) for T in Ts]
best_T = Ts[np.argmin(nlls)]
print(f"\nFound temperature T = {best_T:.3f} (grid search)")


test_logits_stack = []
for name, model in models.items():
    test_logits_stack.append(get_model_logits_on_set(model, imgs_all))
test_logits_stack = np.stack(test_logits_stack, axis=0)
test_ens_logits = test_logits_stack.mean(axis=0)

calib_probs = 1 / (1 + np.exp(-(test_ens_logits / best_T)))

brier_before = brier_score_loss(y_all, ensemble_probs)
brier_after = brier_score_loss(y_all, calib_probs)


def expected_calibration_error(probs, labels, n_bins=10):
    bins = np.linspace(0.0, 1.0, n_bins+1)
    bin_ids = np.digitize(probs, bins) - 1
    ece = 0.0
    for i in range(n_bins):
        mask = bin_ids == i
        if np.sum(mask) == 0:
            continue
        acc = np.mean(labels[mask])
        conf = np.mean(probs[mask])
        ece += (np.sum(mask)/len(probs)) * abs(acc - conf)
    return ece

ece_before = expected_calibration_error(ensemble_probs, y_all, n_bins=10)
ece_after = expected_calibration_error(calib_probs, y_all, n_bins=10)

print(f"Brier before: {brier_before:.4f}, after: {brier_after:.4f}")
print(f"ECE before: {ece_before:.4f}, after: {ece_after:.4f}")


calib_summary = {
    "best_temperature": float(best_T),
    "brier_before": float(brier_before),
    "brier_after": float(brier_after),
    "ece_before": float(ece_before),
    "ece_after": float(ece_after)
}
with open(os.path.join(OUT, "ensemble_calibration.json"), "w") as f:
    json.dump(calib_summary, f, indent=2)
print("=== OUTPUT 3 saved -> ensemble_calibration.json")


from sklearn.calibration import calibration_curve
plt.figure(figsize=(10,4))
plt.subplot(1,2,1)
pt, pp = calibration_curve(y_all, ensemble_probs, n_bins=10)
plt.plot(pp, pt, marker='o'); plt.plot([0,1],[0,1],'--'); plt.title("Reliability (before)")
plt.xlabel("Mean predicted prob"); plt.ylabel("Fraction positives")
plt.subplot(1,2,2)
pt2, pp2 = calibration_curve(y_all, calib_probs, n_bins=10)
plt.plot(pp2, pt2, marker='o'); plt.plot([0,1],[0,1],'--'); plt.title("Reliability (after)")
plt.tight_layout()
plt.savefig(os.path.join(OUT, "reliability_before_after.png"))
plt.close()
print("Saved reliability diagram -> reliability_before_after.png")

def find_last_conv_layer(model):
    for layer in reversed(model.layers):
        if isinstance(layer, tf.keras.layers.Conv2D):
            return layer.name
    return None

def make_gradcam_heatmap(model, img, last_conv_name=None):

    img_batch = np.expand_dims(img, axis=0).astype(np.float32)
    if last_conv_name is None:
        last_conv_name = find_last_conv_layer(model)
    grad_model = tf.keras.models.Model([model.inputs], [model.get_layer(last_conv_name).output, model.output])
    with tf.GradientTape() as tape:
        conv_outputs, preds = grad_model(img_batch)
        loss = preds[:, 0]
    grads = tape.gradient(loss, conv_outputs)
    pooled_grads = tf.reduce_mean(grads, axis=(0,1,2))
    conv_out = conv_outputs[0].numpy()
    pooled = pooled_grads.numpy()
    for i in range(pooled.shape[-1]):
        conv_out[..., i] *= pooled[i]
    heatmap = np.mean(conv_out, axis=-1)
    heatmap = np.maximum(heatmap, 0)
    heatmap = heatmap / (np.max(heatmap) + 1e-8)
    heat_resized = tf.image.resize(heatmap[..., np.newaxis], IMG_SIZE).numpy().squeeze()
    return heat_resized


ens_preds_before = (ensemble_probs >= 0.5).astype(int)
fn_idx = np.where((y_all == 1) & (ens_preds_before == 0))[0]
if len(fn_idx) > 0:
    sample_idx = int(fn_idx[0])
else:

    pos_idx = np.where(y_all == 1)[0]
    sample_idx = int(pos_idx[0]) if len(pos_idx)>0 else 0

sample_img = imgs_all[sample_idx]
print("Grad-CAM sample index chosen:", sample_idx, "true label:", LABELS[y_all[sample_idx]])

gradcam_dir = os.path.join(OUT, "gradcam_all_models")
os.makedirs(gradcam_dir, exist_ok=True)
for name, model in models.items():
    last_conv = find_last_conv_layer(model)
    if last_conv is None:
        print(f"No conv layer found for {name}, skipping Grad-CAM")
        continue
    heat = make_gradcam_heatmap(model, sample_img, last_conv_name=last_conv)
    cmap = plt.get_cmap("jet")
    heat_col = cmap(heat)[:,:,:3]
    img_rgb = np.repeat(sample_img.squeeze()[..., np.newaxis], 3, axis=-1)
    overlay = 0.5*img_rgb + 0.5*heat_col
    overlay = np.clip(overlay, 0, 1)
    plt.figure(figsize=(6,3))
    plt.subplot(1,2,1); plt.imshow(sample_img.squeeze(), cmap='gray'); plt.title(f"{name} input"); plt.axis('off')
    plt.subplot(1,2,2); plt.imshow(overlay); plt.title(f"{name} Grad-CAM"); plt.axis('off')
    fname = os.path.join(gradcam_dir, f"{name}_gradcam_idx{sample_idx}.png")
    plt.savefig(fname); plt.close()
    print("Saved Grad-CAM for", name, "->", fname)

from sklearn.metrics import roc_curve
ensemble_pr_prec, ensemble_pr_rec, _ = precision_recall_curve(y_all, ensemble_probs)
ensemble_pr_auc = auc(ensemble_pr_rec, ensemble_pr_prec)
ensemble_mcc = matthews_corrcoef(y_all, (ensemble_probs >= 0.5).astype(int))
ensemble_bal = balanced_accuracy_score(y_all, (ensemble_probs >= 0.5).astype(int))
ensemble_brier = brier_score_loss(y_all, ensemble_probs)

summary = {
    "per_model": {m["model"]: {"mcc":m["mcc"], "balanced_accuracy":m["balanced_accuracy"], "pr_auc":m["pr_auc"], "accuracy":m["accuracy"]} for m in metrics_list},
    "ensemble": {
        "pr_auc": float(ensemble_pr_auc),
        "mcc": float(ensemble_mcc),
        "balanced_accuracy": float(ensemble_bal),
        "brier_before": float(brier_before),
        "brier_after": float(brier_after),
        "ece_before": float(ece_before),
        "ece_after": float(ece_after),
        "temperature": float(best_T)
    }
}
with open(os.path.join(OUT, "summary_all_metrics.json"), "w") as f:
    json.dump(summary, f, indent=2)
print("=== OUTPUT 5 saved -> summary_all_metrics.json")

for name, model in models.items():
    model.save_weights(os.path.join(OUT, f"{name}.weights.h5"))
print("Saved model weights to:", OUT)

print("\nAll outputs saved under:", OUT)
print("Files example:", os.listdir(OUT)[:10])

import os, random
import numpy as np
import matplotlib.pyplot as plt
import tensorflow as tf
import tensorflow_datasets as tfds
from sklearn.manifold import TSNE
from sklearn.metrics import accuracy_score, confusion_matrix, roc_curve, auc

SEED = 42
np.random.seed(SEED); random.seed(SEED); tf.random.set_seed(SEED)

(ds_tr, ds_test), ds_info = tfds.load("pneumonia_mnist",
                                      split=["train", "test"],
                                      as_supervised=True,
                                      with_info=True)
LABELS = ds_info.features["label"].names  # ['NORMAL','PNEUMONIA']
IMG_SIZE = (64,64)
BATCH = 32

def preprocess(img, lbl):
    img = tf.image.resize(img, IMG_SIZE) / 255.0
    img = tf.reshape(img, [IMG_SIZE[0], IMG_SIZE[1], 1])
    return img, lbl

train_ds = ds_tr.map(preprocess).shuffle(512, seed=SEED).batch(BATCH).prefetch(tf.data.AUTOTUNE)
test_ds_tf = ds_test.map(preprocess).batch(BATCH).prefetch(tf.data.AUTOTUNE)

# collect test arrays
imgs_list, y_list = [], []
for x,y in test_ds_tf:
    imgs_list.append(x.numpy()); y_list.append(y.numpy())
imgs_all = np.concatenate(imgs_list, axis=0)
y_all = np.concatenate(y_list, axis=0)
N = len(y_all)
print("Test set size:", N, "| Labels:", LABELS)

def build_simple_cnn():
    inp = tf.keras.Input(shape=(IMG_SIZE[0],IMG_SIZE[1],1))
    x = tf.keras.layers.Conv2D(32,3,activation='relu',padding='same')(inp)
    x = tf.keras.layers.MaxPooling2D()(x)
    x = tf.keras.layers.Conv2D(64,3,activation='relu',padding='same')(x)
    x = tf.keras.layers.GlobalAveragePooling2D()(x)
    pen = tf.keras.layers.Dense(64, activation='relu', name='penultimate')(x)
    out = tf.keras.layers.Dense(1, activation='sigmoid', name='output')(pen)
    m = tf.keras.Model(inp,out, name='SimpleCNN')
    m.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
    return m

def resnet_block(x, filters):
    y = tf.keras.layers.Conv2D(filters,3,padding='same',activation='relu')(x)
    y = tf.keras.layers.Conv2D(filters,3,padding='same')(y)
    out = tf.keras.layers.Add()([x,y])
    return tf.keras.layers.Activation('relu')(out)

def build_resnet_like():
    inp = tf.keras.Input(shape=(IMG_SIZE[0],IMG_SIZE[1],1))
    x = tf.keras.layers.Conv2D(32,3,activation='relu',padding='same')(inp)
    x = tf.keras.layers.MaxPooling2D()(x)
    x = resnet_block(x, 32)
    x = tf.keras.layers.GlobalAveragePooling2D()(x)
    pen = tf.keras.layers.Dense(64, activation='relu', name='penultimate')(x)
    out = tf.keras.layers.Dense(1, activation='sigmoid', name='output')(pen)
    m = tf.keras.Model(inp,out, name='ResNetLike')
    m.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
    return m

def dense_block(x, filters):
    y = tf.keras.layers.Conv2D(filters, 3, padding='same', activation='relu')(x)
    return tf.keras.layers.Concatenate()([x, y])

def build_densenet_like():
    inp = tf.keras.Input(shape=(IMG_SIZE[0],IMG_SIZE[1],1))
    x = tf.keras.layers.Conv2D(16,3,activation='relu',padding='same')(inp)
    x = dense_block(x,16)
    x = tf.keras.layers.MaxPooling2D()(x)
    x = dense_block(x,32)
    x = tf.keras.layers.GlobalAveragePooling2D()(x)
    pen = tf.keras.layers.Dense(64, activation='relu', name='penultimate')(x)
    out = tf.keras.layers.Dense(1, activation='sigmoid', name='output')(pen)
    m = tf.keras.Model(inp,out, name='DenseNetLike')
    m.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
    return m

def build_vgg_like():
    inp = tf.keras.Input(shape=(IMG_SIZE[0],IMG_SIZE[1],1))
    x = tf.keras.layers.Conv2D(32,3,activation='relu',padding='same')(inp)
    x = tf.keras.layers.Conv2D(32,3,activation='relu',padding='same')(x)
    x = tf.keras.layers.MaxPooling2D()(x)
    x = tf.keras.layers.Conv2D(64,3,activation='relu',padding='same')(x)
    x = tf.keras.layers.GlobalAveragePooling2D()(x)
    pen = tf.keras.layers.Dense(64, activation='relu', name='penultimate')(x)
    out = tf.keras.layers.Dense(1, activation='sigmoid', name='output')(pen)
    m = tf.keras.Model(inp,out, name='VGGLike')
    m.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
    return m

def inception_module(x, f=16):
    p1 = tf.keras.layers.Conv2D(f,1,activation='relu',padding='same')(x)
    p2 = tf.keras.layers.Conv2D(f,3,activation='relu',padding='same')(x)
    p3 = tf.keras.layers.Conv2D(f,5,activation='relu',padding='same')(x)
    p4 = tf.keras.layers.MaxPooling2D(3, strides=1, padding='same')(x)
    p4 = tf.keras.layers.Conv2D(f,1,activation='relu',padding='same')(p4)
    return tf.keras.layers.Concatenate()([p1,p2,p3,p4])

def build_inception_like():
    inp = tf.keras.Input(shape=(IMG_SIZE[0],IMG_SIZE[1],1))
    x = tf.keras.layers.Conv2D(16,3,activation='relu',padding='same')(inp)
    x = inception_module(x,16)
    x = tf.keras.layers.GlobalAveragePooling2D()(x)
    pen = tf.keras.layers.Dense(64, activation='relu', name='penultimate')(x)
    out = tf.keras.layers.Dense(1, activation='sigmoid', name='output')(pen)
    m = tf.keras.Model(inp,out, name='InceptionLike')
    m.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
    return m

models = {
    "SimpleCNN": build_simple_cnn(),
    "ResNet": build_resnet_like(),
    "DenseNet": build_densenet_like(),
    "VGG": build_vgg_like(),
    "Inception": build_inception_like()
}

EPOCHS = 2
histories = {}
for name, model in models.items():
    print(f"\nTraining {name} ...")
    histories[name] = model.fit(train_ds, validation_data=test_ds_tf, epochs=EPOCHS, verbose=1)

probs = {}
preds = {}
accs = {}
for name, model in models.items():
    p = model.predict(imgs_all, batch_size=64, verbose=0).ravel()
    probs[name] = p
    preds[name] = (p >= 0.5).astype(int)
    accs[name] = accuracy_score(y_all, preds[name])
    print(f"{name} accuracy: {accs[name]:.4f}")

ensemble_probs = np.mean(np.stack(list(probs.values()), axis=0), axis=0)
ensemble_preds = (ensemble_probs >= 0.5).astype(int)
print("Ensemble accuracy:", accuracy_score(y_all, ensemble_preds))

def find_last_conv_layer(model):
    for layer in reversed(model.layers):
        if isinstance(layer, tf.keras.layers.Conv2D):
            return layer.name
    return None

def make_gradcam_heatmap(model, img, last_conv_name=None):
    # img: (H,W,1) numpy array scaled [0,1]
    if last_conv_name is None:
        last_conv_name = find_last_conv_layer(model)
    img_batch = np.expand_dims(img, axis=0).astype(np.float32)
    grad_model = tf.keras.models.Model([model.inputs], [model.get_layer(last_conv_name).output, model.output])
    with tf.GradientTape() as tape:
        conv_outputs, preds = grad_model(img_batch)
        loss = preds[:,0]
    grads = tape.gradient(loss, conv_outputs)
    pooled_grads = tf.reduce_mean(grads, axis=(0,1,2)).numpy()
    conv_out = conv_outputs[0].numpy()
    for i in range(pooled_grads.shape[-1]):
        conv_out[..., i] *= pooled_grads[i]
    heatmap = np.mean(conv_out, axis=-1)
    heatmap = np.maximum(heatmap, 0)
    heatmap = heatmap / (np.max(heatmap) + 1e-8)
    heatmap_resized = tf.image.resize(heatmap[..., np.newaxis], IMG_SIZE).numpy().squeeze()
    return heatmap_resized

plt.figure(figsize=(14,10))
i = 1
for name, hist in histories.items():
    plt.subplot(5,2,i); plt.plot(hist.history['loss'], label='train loss')
    if 'val_loss' in hist.history: plt.plot(hist.history['val_loss'], label='val loss')
    plt.title(f"{name} Loss"); plt.legend(); i+=1
    plt.subplot(5,2,i); plt.plot(hist.history.get('accuracy',[]), label='train acc')
    if 'val_accuracy' in hist.history: plt.plot(hist.history.get('val_accuracy',[]), label='val acc')
    plt.title(f"{name} Accuracy"); plt.legend(); i+=1
plt.tight_layout(); plt.show()

fn_idx = np.where((y_all == 1) & (ensemble_preds == 0))[0]
print("Total false negatives (ensemble):", len(fn_idx))
if len(fn_idx) == 0:
    print("No false negatives to show.")
else:
    # sort by lowest ensemble prob (most confident normal but pneumonia)
    order = np.argsort(ensemble_probs[fn_idx])
    topk = fn_idx[order][:12]
    plt.figure(figsize=(12,6))
    for j, idx in enumerate(topk):
        plt.subplot(3,4,j+1)
        plt.imshow(imgs_all[idx].squeeze(), cmap='gray')
        plt.title(f"Idx:{idx}\nProb:{ensemble_probs[idx]:.2f}")
        plt.axis('off')
    plt.suptitle("Top False Negatives (ensemble)")
    plt.tight_layout(); plt.show()

models_for_cam = ["SimpleCNN","VGG","ResNet"]

selected_idxs = []
if len(fn_idx) > 0:
    selected_idxs.extend(fn_idx[:3])

while len(selected_idxs) < 6:
    candidate = int(np.random.randint(0, N))
    if candidate not in selected_idxs:
        selected_idxs.append(candidate)
selected_idxs = selected_idxs[:6]
print("Grad-CAM examples indices:", selected_idxs)

for model_name in models_for_cam:
    model = models[model_name]
    last_conv = find_last_conv_layer(model)
    if last_conv is None:
        print(f"No conv layer found for {model_name}, skipping Grad-CAM for this model.")
        continue
    plt.figure(figsize=(12,8))
    for i, idx in enumerate(selected_idxs):
        heat = make_gradcam_heatmap(model, imgs_all[idx], last_conv_name=last_conv)

        img = imgs_all[idx].squeeze()
        cmap = plt.get_cmap('jet')
        heat_col = cmap(heat)[:,:,:3]
        img_rgb = np.repeat(img[..., np.newaxis], 3, axis=-1)
        overlay = 0.5 * img_rgb + 0.5 * heat_col
        overlay = np.clip(overlay, 0, 1)
        plt.subplot(3,6,i+1 if False else i+1)
        plt.imshow(overlay)
        title = f"Idx:{idx}\nT:{LABELS[y_all[idx]]}\nP:{ensemble_probs[idx]:.2f}"
        plt.title(title); plt.axis('off')
    plt.suptitle(f"Grad-CAM overlays — {model_name}")
    plt.tight_layout(); plt.show()

def extract_penultimate(model, imgs, layer_name='penultimate'):
    inter = tf.keras.Model(inputs=model.input, outputs=model.get_layer(layer_name).output)
    feats = inter.predict(imgs, batch_size=64, verbose=0)
    return feats

# take subset for speed
max_samples = min(300, N)
sel_idx = np.random.choice(np.arange(N), size=max_samples, replace=False)
sel_imgs = imgs_all[sel_idx]
sel_labels = y_all[sel_idx]

feats = extract_penultimate(models["SimpleCNN"], sel_imgs)  # shape (max_samples, dim)
print("Running t-SNE on", feats.shape[0], "samples ...")
tsne = TSNE(n_components=2, perplexity=30, random_state=SEED, init='pca', learning_rate='auto')
feat2d = tsne.fit_transform(feats)

plt.figure(figsize=(7,6))
colors = ['tab:blue' if l==0 else 'tab:orange' for l in sel_labels]
plt.scatter(feat2d[:,0], feat2d[:,1], c=colors, s=12, alpha=0.8)
plt.title("t-SNE of SimpleCNN penultimate features")
plt.xlabel("TSNE1"); plt.ylabel("TSNE2"); plt.show()

grid_k = min(36, N)
plt.figure(figsize=(10,10))
for i in range(grid_k):
    idx = i
    plt.subplot(6,6,i+1)
    plt.imshow(imgs_all[idx].squeeze(), cmap='gray')
    prob = ensemble_probs[idx]
    plt.title(f"P:{prob:.2f}\nT:{LABELS[y_all[idx]]}", fontsize=8)
    plt.axis('off')
    # add small colored rectangle indicating prob (red=high prob, blue=low)
    plt.gca().add_patch(plt.Rectangle((0,0),2,2, transform=plt.gca().transAxes,
                                     facecolor=plt.cm.Reds(prob), alpha=0.6))
plt.suptitle("Ensemble probabilities (first 36 test images)"); plt.tight_layout(); plt.show()

print("Done — 5 visual outputs displayed: learning curves, FN montage, Grad-CAMs, t-SNE, ensemble heatmap.")